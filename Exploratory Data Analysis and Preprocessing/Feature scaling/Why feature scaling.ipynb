{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "knowing-chess",
   "metadata": {},
   "source": [
    "# 1. Why feature scaling?\n",
    "\n",
    "Feature scaling improves the convergence of steepest descent algorithms, which do not possess the property of scale invariance.\n",
    "\n",
    "In stochastic gradient descent training examples inform the weight updates iteratively like so,\n",
    "\n",
    "\\\\( w_{t+1} = w_t - \\gamma\\nabla_w\\ell(f_w(x),y) \\\\)\n",
    "\n",
    "Where\n",
    "- \\\\(w\\\\) - are the weights\n",
    "- \\\\(\\gamma\\\\) - is a stepsize\n",
    "- \\\\(\\nabla\\\\) - is the gradient wrt weights\n",
    "- \\\\(\\ell\\\\) - is the loss function\n",
    "- \\\\(f_w\\\\) - is the function parameterized by \\\\(w\\\\)\n",
    "- \\\\(x\\\\) - is the training example\n",
    "- \\\\(y\\\\) - is the response/label\n",
    "\n",
    "Compare the following convex functions, representing proper scaling and improper scaling\n",
    "\n",
    "![pic1](./figures/pic1.png)\n",
    "\n",
    "A step through one weight update of size Î³ will yield much better reduction in the error in the properly scaled case than the improperly scaled case. Shown below is the direction of \\\\( \\nabla_w\\ell(f_w(x),y)\\\\) of length \\\\(\\gamma\\\\).\n",
    "\n",
    "![pic2](./figures/pic2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-manor",
   "metadata": {},
   "source": [
    "# 2. Shall we perform target/label scaling or not?\n",
    "\n",
    "Based on the discussion above, normalizing the output will not affect shape of \\\\(f\\\\). So it's generally not necessary.\n",
    "\n",
    "But this is not that obvious:\n",
    "\n",
    ">  *A target variable with a large spread of values, in turn, may result in large error gradient values causing weight values to change dramatically, making the learning process unstable.* [*see this reference*](https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/)\n",
    "\n",
    ">  > *for regression problems, it is often desirable to scale or transform both the input and the target variables.* [see this too](https://machinelearningmastery.com/how-to-transform-target-variables-for-regression-with-scikit-learn/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-labor",
   "metadata": {},
   "source": [
    "[I found this helpful](https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0)\n",
    "\n",
    "\\\\( y= wx+b\\\\)\n",
    "\n",
    "differece between scaling \\\\(x\\\\), \\\\(y\\\\) : is that not scaling \\\\(x\\\\) results in much numerical errors in \\\\( wx\\\\) term:\n",
    "\n",
    "![pic3](./figures/pic3.png)\n",
    "\n",
    "Roughly, scaling \\\\(x's\\\\) are of more importance as they big order terms will dominate. So, the scaling of \\\\(y\\\\) is of lower importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-triple",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
